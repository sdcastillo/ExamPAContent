---
title: "Bike Sharing Demand -  Solution"
subtitle: "ExamPA.net Solution to SOA PA 6/18/20"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document: 
    theme: cerulean
    toc: true
    code_folding: show
    toc_float: true
---

```{r setup, message=F, warning=F}
#NO CHANGES NEEDED
# Load needed libraries.
library(ggplot2)
library(dplyr)
library(tidyr)
library(caret)
library(kableExtra) #If needed, install this package.  It's just used for making tables.
library(ExamPAData)

theme_set(theme_minimal())

#function to create html tables
make_table <- function(df, caption = ""){
  df %>% 
    kable(caption = caption) %>% 
    kable_styling(bootstrap_options = "striped", full_width = F)}

#global chunk options
knitr::opts_chunk$set(warning = F, message = F, echo = T, fig.width = 14, fig.height = 8)
```


# Task 1 - Select factor variables.

```{r}
# Task 1 Select which variables will be treated as factor variables.

# Load packages
library(tidyverse)

# Load the data
data.all <- read.csv("December 7 data.csv")

glimpse(data.all)

# Change data type to factor variables as needed
# Replace VARIABLE with the variable name twice. Copy and paste as needed.
data.all$season <- as.factor(data.all$season)
data.all$holiday <- as.factor(data.all$holiday)
data.all$weekday <- as.factor(data.all$weekday)
data.all$weathersit <- as.factor(data.all$weathersit)
data.all$year <- as.factor(data.all$year)

# Rename factor variable level names with descriptive names as needed
# Replace VARIABLE with the variable name once and LEVEL with the level names two or more times. The order should match that given when executing levels(data.all$VARIABLE). Copy and paste as needed.
levels(data.all$holiday) <- c("not_holiday", "holiday")
levels(data.all$weekday) <- c("Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat")
levels(data.all$weathersit) <- c("Clear or cloudy", "Mist", "Snow r rain")
levels(data.all$year) <- c("2011", "2012")

# Relevel the factor variables
# Replace VARIABLE names with a list of factor variables to relevel as needed. Add variable names as needed.
vars <- c("holiday", "weekday", "weathersit", "season", "year")

for (i in vars) {
  table <- as.data.frame(table(data.all[, i]))
  max <- which.max(table[, 2])
  level.name <- as.character(table[max, 1])
  data.all[, i] <- relevel(data.all[, i], ref = level.name)
}
rm(i, max, table, level.name, vars)

# Summarize the data
summary(data.all)
```

# Task 2 - Consider a new variable. 

No additional code provided.

# Task 3 - Write an overview of the data for your actuarial manager.

> The following set of Rmd code snippets may be useful in your data exploration. Include your own work here and then remove or comment out the unused templates.

```{r}
# Means and medians of the target variable split by predictor variables.
library(ggplot2)
# Replace VARIABLE names with a list of factor variables as needed. Add variable names as needed.
vars <- c("weekday", "holiday", "season")
for (i in vars) {
  x <- data.all %>%
    group_by_(i) %>%
    summarise(
      mean = mean(bikes_per_hour),
      median = median(bikes_per_hour),
      n = n()
    )

  print(x)
}
rm(i, x, vars)
```

```{r}
# Correlation matrix for numeric variables
cor.matrix <- cor(data.all[, sapply(data.all, is.numeric)])
print("Correlation Matrix")
cor.matrix
rm(cor.matrix)
```

```{r}
# Scatter plots for numeric variables
# Replace VARIABLE names as needed, with color being optional.
ggplot(data.all, aes(x = temp, y = bikes_per_hour)) + 
  geom_point(shape = 1)
```

```{r}
summary(data.all$temp)
```


```{r}
data.all %>% 
  ggplot(aes(temp)) + 
  geom_histogram()
```


```{r}
# Density charts for numeric variables
data.all %>%
  select_if(is.numeric) %>%
  gather() %>% # Make key value pairs that allows the use of facet_wrap
  ggplot(aes(value)) +
  facet_wrap(~key, scales = "free") +
  geom_density()
```

```{r}
# Bar charts for factor variables
data.all %>%
  select_if(is.factor) %>%
  gather() %>%
  ggplot(aes(value)) +
  facet_wrap(~key, scales = "free") +
  geom_bar() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```

```{r}
# Split histograms for factor variables
# Replace VARIABLE names with a list of factor variables as needed. Add variable names as needed.
vars <- c("weekday", "holiday", "season")
for (i in vars) {
  plot <- ggplot(data.all, aes(
    x = bikes_per_hour,
    group = data.all[, i],
    fill = data.all[, i],
    y = ..density..
  )) +
    geom_histogram(position = "dodge", binwidth = 100) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))

  print(plot)
}
rm(i, plot, vars)
```

```{r}
# Boxplots on target variable, applying factorization to numeric variables as needed
vars <- colnames(data.all)[colnames(data.all) != "bikes_per_hour"]
for (i in vars) {
  plot <- ggplot(data.all, aes(x = as.factor(data.all[, i]), y = bikes_per_hour)) +
    geom_boxplot() +
    labs(x = i) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
  print(plot)
}
rm(i, vars, plot)
```



# Task 4 - Select an interaction to consider for your model. 

No additional code provided. Use Task 3 code snippets to explore for interaction as appropriate.


```{r}
# Boxplots on target variable by two factor variables
# Replace VARIABLE1 and VARIABLE2 with different factor variables. Copy and paste as needed.
ggplot(data = data.all, aes(
  x = weathersit, y = bikes_per_hour,
  fill = weekday
)) +
  geom_boxplot(alpha = 0.5) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
```

```{r}
data.all %>% 
  mutate(weekend = ifelse(weekday %in% c("Sat", "Sun"), "Weekend", "Weekday")) %>% 
  group_by(weathersit, weekend) %>% summarise(median  = median(bikes_per_hour))
```

# Task 5 - Perform a k-means cluster analysis.

The following is the analysis the assistant provided. Do not change this code.

```{r}
# Create data.num with a subset of numeric variables for cluster analysis and scale the variables.

data.num <- data.all[, c("temp", "humidity")]
data.num$temp <- scale(data.num$temp)
data.num$humidity <- scale(data.num$humidity)

# k-means fitting and graph preparation
set.seed(200)

km1 <- kmeans(data.num, 1, nstart = 10)
data.num$group <- as.factor(km1$cluster)
p1 <- ggplot(data = data.num, aes(x = temp, y = humidity, col = group)) +
  geom_point() +
  ggtitle("k=1")

km2 <- kmeans(data.num, 2, nstart = 10)
data.num$group <- as.factor(km2$cluster)
p2 <- ggplot(data = data.num, aes(x = temp, y = humidity, col = group)) +
  geom_point() +
  ggtitle("k=2")

km3 <- kmeans(data.num, 3, nstart = 10)
data.num$group <- as.factor(km3$cluster)
p3 <- ggplot(data = data.num, aes(x = temp, y = humidity, col = group)) +
  geom_point() +
  ggtitle("k=3")

km4 <- kmeans(data.num, 4, nstart = 10)
data.num$group <- as.factor(km4$cluster)
p4 <- ggplot(data = data.num, aes(x = temp, y = humidity, col = group)) +
  geom_point() +
  ggtitle("k=4")

km5 <- kmeans(data.num, 5, nstart = 10)
data.num$group <- as.factor(km5$cluster)
p5 <- ggplot(data = data.num, aes(x = temp, y = humidity, col = group)) +
  geom_point() +
  ggtitle("k=5")

km6 <- kmeans(data.num, 6, nstart = 10)
data.num$group <- as.factor(km6$cluster)
p6 <- ggplot(data = data.num, aes(x = temp, y = humidity, col = group)) +
  geom_point() +
  ggtitle("k=6")

# k-means elbow plot
var.exp <- data.frame(
  k = c(1:6),
  bss_tss = c(
    km1$betweenss / km1$totss,
    km2$betweenss / km2$totss,
    km3$betweenss / km3$totss,
    km4$betweenss / km4$totss,
    km5$betweenss / km5$totss,
    km6$betweenss / km6$totss
  )
)
ggplot(data = var.exp, aes(x = k, y = bss_tss)) +
  geom_point() +
  geom_line() +
  ggtitle("Elbow Plot")
rm(var.exp)

# k-means graphs
library(gridExtra)
grid.arrange(p1, p2, ncol = 2)
grid.arrange(p3, p4, ncol = 2)
grid.arrange(p5, p6, ncol = 2)

```

The following creates a cluster variable based on your selection.

```{r}
# Create data.clustered$cluster to house new cluster variable
data.clustered <- data.all

# Add cluster variable.
# Change km_ to one of km1, km2, km3, km4, km5, or km6 based on selected parameter k
data.clustered$cluster <- as.factor(km3$cluster)

# Remove variables used to create the cluster value
data.clustered$temp <- NULL
data.clustered$humidity <- NULL

# Remove unneeded cluster variables
rm(km1, km2, km3, km4, km5, km6)

# Summary of new variable to check work
print("Summary of new variable")
summary(data.clustered$cluster)
```

# Task 6 - Construct a decision tree.

Split the data into training and test data sets. Do not change the code.

```{r}
# Split the data into training and test data sets.
library(caret)
set.seed(1000)

# Split data.all into training and test data sets.
train_ind <- createDataPartition(data.all$bikes_per_hour, p = 0.7, list = FALSE)
data.train <- data.all[train_ind, ]
data.test <- data.all[-train_ind, ]

# split data.clustered into training and test data sets.
data.clustered.train <- data.clustered[train_ind, ]
data.clustered.test <- data.clustered[-train_ind, ]

# Compare means to validate data split
print("MEANS")
print("TRAIN")
mean(data.train$bikes_per_hour)

print("TEST")
mean(data.test$bikes_per_hour)

print("ALL")
mean(data.all$bikes_per_hour)

# Remove unneeded variable
rm(train_ind)
```

Construct an unpruned regression tree. Do not change the code.

```{r}
# Load needed packages and set seed
library(rpart)
library(rpart.plot)
set.seed(555)

# Fit the model
tree1 <- rpart(bikes_per_hour ~ .,
  data = data.train,
  method = "anova",
  control = rpart.control(cp = 0.001, minbucket = 20)
)

# Print output for the tree
tree1

# Plot the tree
rpart.plot(tree1, tweak = 0.8)
```

Display the complexity parameter table and plot.

```{r}
# Display the complexity parameter table and plot for tree1.
tree1$cptable
plotcp(tree1)
```

Prune the tree as instructed and display it.

```{r}
# Prune the tree using the complexity parameter that will result in four leaves.
# Replace xx with an appropriate value
tree2 <- prune(tree1, cp = 0.042345685)

# Print a summary of the tree
tree2

# Plot the tree
prp(tree2,
  yesno = 2,
  extra = 100
)
```

# Task 7 - Construct a boosted decision tree. 

Construct a boosted tree with given parameters and shrinkage of 0.01. Do not change the code.

```{r}
# Fit boosted tree
library(gbm)
boost.tree1 <- gbm(bikes_per_hour ~ .,
  data = data.train,
  distribution = "gaussian",
  n.trees = 1000,
  shrinkage = 0.01,
  interaction.depth = 4
) # number of total splits in each tree
summary(boost.tree1)

# Make predictions from boosted decision tree using a number of fitted trees ranging from 20 to 1000
n.trees <- seq(from = 20, to = 1000, by = 20)
predbytrees <- predict(boost.tree1, newdata = data.test, n.trees = n.trees)

# Plot the MSE for each decision tree
boost.err <- with(data.test, apply((predbytrees - bikes_per_hour)^2, 2, mean))
plot(n.trees, boost.err, pch = 23, ylab = "Mean Squared Error", xlab = "# Trees", main = "Boosting Test Error")
```

Plot partial dependence plots on two variables.

```{r}
# Plot partial dependence plots on a variable
# Replace VARIABLE with the desired variable. Copy and paste as needed.
library(pdp)
partial(boost.tree1, n.trees = 1000, pred.var = "year", plot = TRUE)
partial(boost.tree1, n.trees = 1000, pred.var = "weekday", plot = TRUE)
```

Construct a boosted tree with given parameters and shrinkage of 0.1. Do not change the code.

```{r}
# Fit boosted tree
library(gbm)
boost.tree2 <- gbm(bikes_per_hour ~ .,
  data = data.train,
  distribution = "gaussian",
  n.trees = 1000,
  shrinkage = 0.1,
  interaction.depth = 4
) # number of total splits in each tree
summary(boost.tree2)

# Make predictions from boosted decision tree using a number of fitted trees ranging from 20 to 1000
n.trees <- seq(from = 20, to = 1000, by = 20)
predbytrees <- predict(boost.tree2, newdata = data.test, n.trees = n.trees)

# Plot the MSE for each decision tree
boost.err <- with(data.test, apply((predbytrees - bikes_per_hour)^2, 2, mean))
plot(n.trees, boost.err, pch = 23, ylab = "Mean Squared Error", xlab = "# Trees", main = "Boosting Test Error")
```

Plot partial dependence plots on two variables.

```{r}
# Plot partial dependence plots on a variable
# Replace VARIABLE with the desired variable. Copy and paste as needed.
#partial(boost.tree2, n.trees = 1000, pred.var = "VARIABLE", plot = TRUE)
#partial(boost.tree2, n.trees = 1000, pred.var = "VARIABLE", plot = TRUE)
```

Compare the predictive power of the two trees. Do not change this code.

```{r}
# Predict on train data
tree1.pred.train <- predict(boost.tree1, newdata = data.train, n.trees = 1000)
tree2.pred.train <- predict(boost.tree2, newdata = data.train, n.trees = 1000)

# Predict on test data
tree1.pred.test <- predict(boost.tree1, newdata = data.test, n.trees = 1000)
tree2.pred.test <- predict(boost.tree2, newdata = data.test, n.trees = 1000)

# Calculate the MSE to compare trees
print("Mean Squared Error")
print("Shrinkage at 0.01, Train")
sum((data.train$bikes_per_hour - tree1.pred.train)^2) / nrow(data.train) # Train MSE
print("Shrinkage at 0.01, Test")
sum((data.test$bikes_per_hour - tree1.pred.test)^2) / nrow(data.test) # Test MSE
print("Shrinkage at 0.1, Train")
sum((data.train$bikes_per_hour - tree2.pred.train)^2) / nrow(data.train)
print("Shrinkage at 0.1, Test")
sum((data.test$bikes_per_hour - tree2.pred.test)^2) / nrow(data.test)
```

# Task 8 - Compare distribution choices for a generalized linear model (GLM).

Fit the GLM using Poisson. Do not change this code.

```{r}
# Construct GLM with all variables using original data and a Poisson distribution with canonical link function.
glm.poisson <- glm(bikes_per_hour ~ ., family = poisson(link = "log"), data = data.train)
summary(glm.poisson)

# Predict on train and test data
glm.poisson.pred.train <- predict(glm.poisson, newdata = data.train, type = "response")
glm.poisson.pred.test <- predict(glm.poisson, newdata = data.test, type = "response")

# Calculate the MSE
print("Mean Squared Error")
print("Poisson, Train")
sum((data.train$bikes_per_hour - glm.poisson.pred.train)^2) / nrow(data.train) # Train MSE
print("Poisson, Test")
sum((data.test$bikes_per_hour - glm.poisson.pred.test)^2) / nrow(data.test) # Test MSE
```

Fit the GLM using gamma. Do not change this code.

```{r}
# Construct GLM with all variables using original data and a gamma distribution with canonical link function
glm.gamma <- glm(data.train$bikes_per_hour ~ ., family = Gamma(link = "inverse"), data = data.train)
summary(glm.gamma)

# Predict on train and test data
glm.gamma.pred.train <- predict(glm.gamma, newdata = data.train, type = "response")
glm.gamma.pred.test <- predict(glm.gamma, newdata = data.test, type = "response")

# Calculate the MSE
print("Mean Squared Error")
print("Gamma, Train")
sum((data.train$bikes_per_hour - glm.gamma.pred.train)^2) / nrow(data.train) # Train MSE
print("Gamma, Test")
sum((data.test$bikes_per_hour - glm.gamma.pred.test)^2) / nrow(data.test) # Test MSE
```

# Task 9 - Evaluate the interaction term.

Fit the GLM without the interaction term. Do not change this code.

```{r}
# Construct GLM with all original variables, using a Poisson distribution
glm.base <- glm(bikes_per_hour ~ ., family = poisson(link = "log"), data = data.train)
summary(glm.base)

# Predict on train and test data
glm.base.pred.train <- predict(glm.base, newdata = data.train, type = "response")
glm.base.pred.test <- predict(glm.base, newdata = data.test, type = "response")

# Calculate the MSE
print("Mean Squared Error")
print("Base, Train")
sum((data.train$bikes_per_hour - glm.base.pred.train)^2) / nrow(data.train) # Train MSE
print("Base, Test")
sum((data.test$bikes_per_hour - glm.base.pred.test)^2) / nrow(data.test) # Test MSE
```

Fit the GLM with the interaction term.

```{r}
# Construct GLM with all original variables but adding interaction, using a Poisson distribution
# Replace VARIABLE1 and VARIABLE2 with the appropriate variable names
glm.interaction <- glm(bikes_per_hour ~ . + weathersit * weekday, family = poisson(link = "log"), data = data.train)
summary(glm.interaction)

# Predict on train and test data
glm.interaction.pred.train <- predict(glm.interaction, newdata = data.train, type = "response")
glm.interaction.pred.test <- predict(glm.interaction, newdata = data.test, type = "response")

# Calculate the MSE
print("Mean Squared Error")
print("With Interaction, Train")
sum((data.train$bikes_per_hour - glm.interaction.pred.train)^2) / nrow(data.train) # Train MSE
print("With Interaction, Test")
sum((data.test$bikes_per_hour - glm.interaction.pred.test)^2) / nrow(data.test) # Test MSE
```

# Task 10 - Evaluate the cluster variable in the GLM.

Fit the GLM with the cluster variable. Do not change this code.

```{r}
# Construct GLM with all variables and a Poisson distribution on clustered data.
# Clustered data includes the cluster variable and excludes the variables used to create the clustered variable.

glm.clustered <- glm(bikes_per_hour ~ ., family = poisson(link = "log"), data = data.clustered.train)
summary(glm.clustered)

# Predict on train and test data
glm.clustered.pred.train <- predict(glm.clustered, newdata = data.clustered.train, type = "response")
glm.clustered.pred.test <- predict(glm.clustered, newdata = data.clustered.test, type = "response")

# Calculate the MSE
print("Mean Squared Error")
print("With Cluster, Train")
sum((data.clustered.train$bikes_per_hour - glm.clustered.pred.train)^2) / nrow(data.clustered.train) # Train MSE
print("With Cluster, Test")
sum((data.clustered.test$bikes_per_hour - glm.clustered.pred.test)^2) / nrow(data.clustered.test) # Test MSE
```

# Task 11 - Select the final model to present to the client.

This runs an intercept-only model. Do not change this code.

```{r}
# Construct GLM with only intercept term.
glm.intercept <- glm(bikes_per_hour ~ 1, family = poisson(link = "log"), data = data.train)
summary(glm.intercept)

# Predict on train and test data
glm.intercept.pred.train <- predict(glm.intercept, newdata = data.train, type = "response")
glm.intercept.pred.test <- predict(glm.intercept, newdata = data.test, type = "response")

# Calculate the MSE
print("Mean Squared Error")
print("Intercept Only, Train")
sum((data.train$bikes_per_hour - glm.intercept.pred.train)^2) / nrow(data.train) # Train MSE
print("Intercept Only, Test")
sum((data.test$bikes_per_hour - glm.intercept.pred.test)^2) / nrow(data.test) # Test MSE
```

# Task 12 - Write an executive summary for the client.

No additional code provided.
