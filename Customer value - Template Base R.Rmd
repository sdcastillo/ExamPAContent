---
title: "Customer value - template"
editor_options:
  chunk_output_type: inline
---

Your assistant has provided the following code to load the dataset and provide summary values. It also changes the target variable from "High" and "Low" to 1 and 0. It also loads four libraries that will be used.

```{r}
# Loading data
library(ExamPAData)
library(tidyverse)
#this loads the data and converts characters to factors 
#the read.csv function loads as factors by default, so this works best with base R
df <- customer_value %>% 
  mutate(value_flag = ifelse(value_flag=="High",1,0)) %>% 
  mutate_if(is.character, as.factor)


library(ggplot2)
library(caret)
glimpse(df)
```

Your assistant has provided the following code that contains items that may be helpful.

```{r eval = F}
# In each case, replace the word variable with the name of the variable to analyze.

# This code makes a histogram for a continuous variable.
ggplot(df, aes(x = variable)) + 
  geom_histogram(bins = 30) +
  labs(x = "variable") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# This code makes a bar chart for a factor variable.
ggplot(df, aes(x = variable)) +
  geom_bar() +
  labs(x = "variable") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# This code provides, for each level of a factor variable, the number for which value_flag is zero, the number for which it is one, the total number of observations, and the proportion of ones.
# Note that the variable name should not be enclosed in quotation marks.
df %>%
  group_by(variable) %>%
  summarise(
    zeros = sum(value_flag == 0),
    ones = sum(value_flag == 1),
    n = n(),
    proportion = mean(value_flag)
  )

# This code provides separate histograms for a continuous variable, one for those with value_flag = 0 and one for those with value_flag = 1. Bins may need to be changed.
ggplot(
  df,
  aes(
    x = variable,
    group = value_flag,
    fill = as.factor(value_flag),
    y = ..density..
  )
) +
  geom_histogram(position = "dodge", bins = 30)

# This code relevels a factor variable so that the base level is the one with the most observations. Note that if the identity of the level with the most observations is known, only the last command is needed. That command could also be used to set any level as the base.
table <- as.data.frame(table(df$variable))
max <- which.max(table[, 2])
level.name <- as.character(table[max, 1])
df$variable <- relevel(df$variable, ref = level.name)

# This code removes all observations where the value of a numeric variable exceeds 50.
df <- df[df$variable <= 50, ]

# This code removes all observations of a factor variable that have a value equal to "value".
df <- df[df$variable != "value", ]

# This code takes a continuous variable and creates a binned factor variable. The code applies it directly to the capital gain variable as an example. right = FALSE means that the left number is included and the right number excluded. So, in this case, the first bin runs from 0 to 1000 and includes 0 and excludes 1000. Note that the code creates a new variable, so the original variable is retained.
df$cap_gain_cut <- cut(df$cap_gain, breaks = c(0, 1000, 5000, Inf), right = FALSE, labels = c("lowcg", "mediumcg", "highcg"))

# This code takes a factor variable and combines factor levels into new factors. The occupation variable is used as an example. Note that the code creates a new variable, so the original variable is retained.
var.levels <- levels(df$occupation)
df$occupation_comb <- mapvalues(df$occupation, var.levels, c("Group12", "Group12", "Group345", "Group345", "Group345", "GroupNA"))

# This code removes a variable from the dataframe.

df$variable <- NULL

# This code creates training and testing sets.

set.seed(22)
train_ind <- createDataPartition(df$value_flag, p = 0.7, list = FALSE)
data.train <- df[train_ind, ]
data.test <- df[-train_ind, ]
print("TRAIN")
mean(data.train$value_flag)
print("TEST")
mean(data.test$value_flag)
rm(train_ind)
```

TASK 1

```{r}
glimpse(df)
summary(df)

# This code makes a histogram for a continuous variable.
ggplot(df, aes(x = age)) + 
  geom_histogram(bins = 30) +
  labs(x = "age") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

df <- df %>% filter(age>=25)
df <- df %>% mutate(log_age = log(age))

ggplot(
  df,
  aes(
    x = log_age,
    group = value_flag,
    fill = as.factor(value_flag),
    y = ..density..
  )
) +
  geom_histogram(position = "dodge", bins = 30)
```

```{r}
ggplot(df, aes(x = education_num)) + 
  geom_histogram(bins = 30) +
  labs(x = "education_num") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

ggplot(
  df,
  aes(
    x = education_num,
    group = value_flag,
    fill = as.factor(value_flag),
    y = ..density..
  )
) +
  geom_histogram(position = "dodge", bins = 30)
```

```{r}
# This code provides, for each level of a factor variable, the number for which value_flag is zero, the number for which it is one, the total number of observations, and the proportion of ones.
# Note that the variable name should not be enclosed in quotation marks.
df %>%
  group_by(marital_status) %>%
  summarise(
    zeros = sum(value_flag == 0),
    ones = sum(value_flag == 1),
    n = n(),
    proportion = mean(value_flag)
  )

df <- df %>% mutate(marital_status = ifelse(marital_status %in% c("Married-AF-spouse", "Married-civ-spouse", "Married-spouse-absent"), "Married", marital_status))

df %>% count(marital_status)
```

```{r}
df %>%
  group_by(occupation) %>%
  summarise(
    zeros = sum(value_flag == 0),
    ones = sum(value_flag == 1),
    n = n(),
    proportion = mean(value_flag)
  )
```
```{r}
ggplot(df, aes(x = hours_per_week)) + 
  geom_histogram(bins = 30) +
  labs(x = "hours_per_week") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

ggplot(
  df,
  aes(
    x = hours_per_week,
    group = value_flag,
    fill = as.factor(value_flag),
    y = ..density..
  )
) +
  geom_histogram(position = "dodge", bins = 30)

summary(df$hours_per_week)
```

```{r}
ggplot(df, aes(x = score)) + 
  geom_histogram(bins = 30) +
  labs(x = "hours_per_week") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

ggplot(
  df,
  aes(
    x = score,
    group = value_flag,
    fill = as.factor(value_flag),
    y = ..density..
  )
) +
  geom_histogram(position = "dodge", bins = 30)

summary(df$score)
```

Set reference levels

```{r}
df <- df %>% select(-age) %>% mutate_if(is.character, fct_infreq)
summary(df)
```


No code

TASK 2

```{r}
set.seed(22)
train_ind <- createDataPartition(df$value_flag, p = 0.7, list = FALSE)
data.train <- df[train_ind, ]
data.test <- df[-train_ind, ]
print("TRAIN")
mean(data.train$value_flag)
print("TEST")
mean(data.test$value_flag)
rm(train_ind)
```


This first chunk constructs a decision tree by allowing parameters to be set and then provides results. 

```{r}
library(rpart)
library(rpart.plot)
set.seed(1234)

# Keep maxdepth at 6 or less. The other parameters can be changed as desired to produce the final tree.

tree1 <- rpart(as.factor(value_flag) ~ .,
  data = data.train,
  method = "class",
  control = rpart.control(minbucket = 3, cp = 0.001, maxdepth = 6),
  parms = list(split = "gini")
)

tree1
rpart.plot(tree1)

# See how well it fits the data used to train the model.
pred <- predict(tree1, type = "class", data = data.train)
confusionMatrix(pred, factor(data.train$value_flag))

# See how well the model performs on the test data.
pred.test <- predict(tree1, type = "class", newdata = data.test)
confusionMatrix(pred.test, factor(data.test$value_flag))

# Construct ROC and calculate AUC for the training data.
library(pROC)

preds.prob <- predict(tree1, type = "prob", data = data.train)

roc <- roc(data.train$value_flag, preds.prob[, 2])

par(pty = "s") # This improves the roc plot

plot(roc)
pROC::auc(roc) # The addition of pROC:: to the code ensures that the auc function from that package is used. Other packages also have this function.

# Construct ROC and calculate AUC for the test data.

preds.prob.test <- predict(tree1, type = "prob", newdata = data.test)

roc <- roc(data.test$value_flag, preds.prob.test[, 2])

plot(roc)

pROC::auc(roc)
```

This chunk does the same but uses cross-validation to set the cp value. 

```{r}
# This set up for y (the target) and x (the predictors) produces a more accurate model. If any changes were made to variable names, the list after varlist <- should be edited.

set.seed(6)

target <- factor(data.train$value_flag)
varlist <- c("log_age", "education_num", "marital_status", "occupation", "cap_gain", "hours_per_week", "score")
predictors <- data.train[, varlist]

Grid <- expand.grid(cp = seq(0, 0.01, 0.0005)) # sets the range of cp values to check rather than have the program use default values.

fitControl <- trainControl(method = "cv", number = 6) # Do not change this
tree2 <- train(
  y = target,
  x = predictors,
  method = "rpart",
  trControl = fitControl,
  control = rpart.control(maxdepth = 6), # Do not use a value greater than 6.
  metric = "Accuracy",
  tuneGrid = Grid,
  na.action = na.omit,
  parms = list(split = "gini")
)
tree2$results
tree2$finalModel

plot(tree2)
rpart.plot(tree2$finalModel, extra = 4)

pred2 <- predict(tree2, type = "raw")
confusionMatrix(pred2, factor(data.train$value_flag))

pred2.test <- predict(tree2, type = "raw", newdata = data.test)
confusionMatrix(pred2.test, factor(data.test$value_flag))

# Construct ROC and calculate AUC for the training data.

pred2.prob <- predict(tree2, type = "prob", data = data.train)

roc <- roc(data.train$value_flag, pred2.prob[, 2])
par(pty = "s")
plot(roc)

pROC::auc(roc)

# Construct ROC and calculate AUC for the test data.

pred2.prob.test <- predict(tree2, type = "prob", newdata = data.test)

roc <- roc(data.test$value_flag, pred2.prob.test[, 2])

plot(roc)

pROC::auc(roc)

tree2$finalModel$xNames
```

TASK 3

This chunk runs a random forest. Do not change any of the parameters.

```{r}
set.seed(42)
# Note that this code uses the same target and predictors as set up in the cv approach in Task 2.

# Set up the grid.
rfGrid <- expand.grid(mtry = c(1:3))

# Set the controls.
ctrl <- trainControl(
  method = "cv",
  number = 4
)

# Train the model.
rf <- train(
  y = target,
  x = predictors,
  method = "rf",
  trControl = ctrl,
  tuneGrid = rfGrid,
  ntree = 50,
  nodesize = 5,
  maxdepth = 5,
  importance = TRUE
)

# View the output.
rf
plot(rf)

# Obtain the confusion matrices.
pred.rf <- predict(rf, predictors)

confusionMatrix(pred.rf, factor(data.train$value_flag))

pred.rf.test <- predict(rf, newdata = data.test)

confusionMatrix(pred.rf.test, factor(data.test$value_flag))

# Obtain the importance of the features.
varImp(rf)

# Construct ROC and calculate AUC for the training data.

predrf.prob <- predict(rf, type = "prob", data = data.train)

roc <- roc(data.train$value_flag, predrf.prob[, 2])
par(pty = "s")
plot(roc)

pROC::auc(roc)

# Construct ROC and calculate AUC for the test data.

predrf.prob.test <- predict(rf, type = "prob", newdata = data.test)

roc <- roc(data.test$value_flag, predrf.prob.test[, 2])

plot(roc)

pROC::auc(roc)
```

TASK 4

No Code

TASK 5

No code

TASK 6

No code

TASK 7

No code

TASK 8

Construct and evaluate a GLM

```{r}

glm <- glm(value_flag ~ VARIABLE_LIST,
  data = data.train,
  family = DISTRIBUTION(link = "LINK")
)
# Replace VARIABLE LIST with selected variables, DISTRIBUTION with the distribution choice, and LINK with the link choice

summary(glm)

glm.probs <- predict(glm, data.train, type = "response")
glm.pred <- glm.probs > 0.5
table(glm.pred, data.train$value_flag)

glm.probs.test <- predict(glm, data.test, type = "response")
glm.pred.test <- glm.probs.test > 0.5
table(glm.pred.test, data.test$value_flag)

# Construct ROC and calculate AUC for the training data.

roc <- roc(data.train$value_flag, glm.probs)
par(pty = "s")
plot(roc)

pROC::auc(roc)

# Construct ROC and calculate AUC for the test data.

roc <- roc(data.test$value_flag, glm.probs.test)

plot(roc)

pROC::auc(roc)
```

The following code executes the stepAIC procedure allowing for choices of AIC/BIC and forward/backward.

```{r}
# This code runs the stepAIC procdure. It is set up to use the GLM model previously fit along with forward #selection and BIC. That does not imply these are the best choices.

# Note: When using BIC, decisions about adding or removing variables are made using that criterion. However, the #AIC value presented at the end when the final model is run uses the standard AIC penalty of k = 2.

library(MASS)
# If using forward selection it is necessary to fit a model with no predictors to use as the start.
glm.none <- glm(value_flag ~ 1, data = data.train, family = FAMILY(link = "LINK"))

stepAIC(glm.none,
  direction = "forward",
  k = log(nrow(data.train)),
  scope = list(upper = glm, lower = glm.none) # set k = 2 for AIC
)

# For backward, replace the very first instance of glm.none with glm.
```

To evaluate the selected model, copy previous code, changing the variable list to those selected by stepAIC.

Task 9

No Code

Task 10

This code calculates the expected profit based on a selected cutoff for predicting a high value customer. The instructions will guide you to work with whichever model was selected as the final model. It assumes the code for that model has been run and predictions stored in the variable used in the given code.

```{r}
cutoff <- 0.5 # set the cutoff

# Get the predicted probabilities of being high value in a vector. Remove the comment indicator from the one you want to use.
# predict.prob <- preds.prob.test[, 2] #For using the first tree
# predict.prob <- pred2.prob.test[, 2] #For using the tree with cp set by cross validation
# predict.prob <- predrf.prob.test[, 2] #For using the random forest
# predict.prob <- glm.probs.test #For using the final GLM.

pred.one <- 1 * (predict.prob > cutoff)
cm <- confusionMatrix(factor(pred.one), factor(data.test$value_flag))
cm$table
Market.to.high <- 50 * cm$table[2, 2]
Market.to.low <- -25 * cm$table[2, 1]
Do.not.market <- -5 * (cm$table[1, 1] + cm$table[1, 2])
Profit <- Market.to.high + Market.to.low + Do.not.market
Profit
```

Task 11

This task provides a data frame with sample cases. The first case is a baseline case with the (rounded) average value (for numeric variables) or the modal value (for factor variables). Each subsquent row has one variable changed to an alternative value.

This data frame can be combined with the appropriate predict function for your model to provide illustrative predictions.

Note for the predict function to work the sample.data data frame must include the same variables as used in the data frame from which your selected model was developed.

```{r}
age_vec <- c(39, 53, 39, 39, 39, 39, 39, 39)
education_vec <- c(10, 10, 13, 10, 10, 10, 10, 10)
martial_vec <- c(
  "Married-civ-spouse", "Married-civ-spouse", "Married-civ-spouse",
  "Never-married", "Married-civ-spouse", "Married-civ-spouse",
  "Married-civ-spouse", "Married-civ-spouse"
)
occupation_vec <- c(
  "Group 3", "Group 3", "Group 3", "Group 3", "Group 5",
  "Group 3", "Group 3", "Group 3"
)
cap_gain_vec <- c(0, 0, 0, 0, 0, 2000, 0, 0)
hours_vec <- c(40, 40, 40, 40, 40, 40, 50, 40)
score_vec <- c(60, 60, 60, 60, 60, 60, 60, 64)
sample.data <- data.frame(
  "age" = age_vec,
  "education_num" = education_vec,
  "marital_status" = martial_vec,
  "occupation" = occupation_vec,
  "cap_gain" = cap_gain_vec,
  "hours_per_week" = hours_vec,
  "score" = score_vec
)
sample.data
```

